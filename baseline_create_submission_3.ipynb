{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba8275f5-03cd-4ca4-9b6a-186e6fe71d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21b9bef6-7a5f-4a3d-bdf8-887a033ec217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Tuple\n",
    "\n",
    "import hydra\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import DictConfig\n",
    "from oml.const import TCfg\n",
    "from oml.datasets.images import get_retrieval_images_datasets\n",
    "from oml.lightning.callbacks.metric import MetricValCallback\n",
    "from oml.lightning.modules.extractor import ExtractorModule, ExtractorModuleDDP\n",
    "from oml.lightning.pipelines.parser import (\n",
    "    check_is_config_for_ddp,\n",
    "    parse_logger_from_config,\n",
    "    parse_ckpt_callback_from_config,\n",
    "    parse_engine_params_from_config,\n",
    "    parse_sampler_from_config,\n",
    "    parse_scheduler_from_config,\n",
    ")\n",
    "from oml.metrics.embeddings import EmbeddingMetrics\n",
    "from oml.registry.losses import get_criterion_by_cfg\n",
    "from oml.registry.models import get_extractor_by_cfg\n",
    "from oml.registry.optimizers import get_optimizer_by_cfg\n",
    "from oml.registry.transforms import TRANSFORMS_REGISTRY, get_transforms_by_cfg\n",
    "from oml.utils.misc import dictconfig_to_dict, set_global_seed\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from oml.const import MEAN, PAD_COLOR, STD, TNormParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "433eb7ad-b0c3-4852-a133-eb0acf3ff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "postfix = \"metric_learning\"\n",
    "\n",
    "current_dateTime = datetime.now()\n",
    "y = current_dateTime.year\n",
    "month = current_dateTime.month\n",
    "d = current_dateTime.day\n",
    "hour = current_dateTime.hour\n",
    "minute = current_dateTime.minute\n",
    "s = current_dateTime.second\n",
    "ms = current_dateTime.microsecond\n",
    "\n",
    "cfg: TCfg = {\n",
    "    \"postfix\": postfix,\n",
    "    \"seed\": 42,\n",
    "    \"image_size\": 224,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"devices\": 1, \n",
    "    \"num_workers\": 4,\n",
    "    \"cache_size\": 0,\n",
    "    \"test_data_dir\": \"../test/gallery/\",\n",
    "    \"bs_val\": 8,  \n",
    "\n",
    "    \"extractor\":{\n",
    "        \"name\": \"vit\",\n",
    "        \"args\":{\n",
    "            \"arch\": \"vitl14\",\n",
    "            # \"gem_p\": 1.0,\n",
    "            # \"remove_fc\": True,\n",
    "            \"normalise_features\": False,\n",
    "            \"weights\": \"checkpoints/best-v4.ckpt\",\n",
    "        },\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b526a378-7163-4272-8f71-84115ffc32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(im_size: int, mean: TNormParam = MEAN, std: TNormParam = STD) -> albu.Compose:\n",
    "    \"\"\"\n",
    "    Use default oml albu augs, but without HorizontalFlip.\n",
    "    :param im_size:\n",
    "    :param mean:\n",
    "    :param std:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return albu.Compose(\n",
    "        [\n",
    "            albu.LongestMaxSize(max_size=im_size),\n",
    "            albu.PadIfNeeded(\n",
    "                min_height=im_size,\n",
    "                min_width=im_size,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=PAD_COLOR,\n",
    "            ),\n",
    "            albu.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfef57",
   "metadata": {},
   "source": [
    "# Формирование предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70e05e79-6acf-4001-b4f9-46b381b8430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from oml.const import IMAGE_EXTENSIONS\n",
    "from oml.datasets.images import ImageBaseDataset\n",
    "from oml.ddp.utils import get_world_size_safe, is_main_process, sync_dicts_ddp\n",
    "from oml.transforms.images.utils import get_im_reader_for_transforms\n",
    "from oml.utils.images.images import find_broken_images\n",
    "from oml.utils.misc import dictconfig_to_dict\n",
    "\n",
    "\n",
    "def extractor_prediction_pipeline(cfg: TCfg) -> None:\n",
    "    \"\"\"\n",
    "    This pipeline allows you to save features extracted by a feature extractor.\n",
    "\n",
    "    \"\"\"\n",
    "    print(cfg)\n",
    "\n",
    "    transforms = get_transforms(cfg['image_size'])\n",
    "    filenames = [list(Path(cfg[\"test_data_dir\"]).glob(f\"**/*.{ext}\")) for ext in IMAGE_EXTENSIONS]\n",
    "    filenames = list(itertools.chain(*filenames))\n",
    "\n",
    "    if len(filenames) == 0:\n",
    "        raise RuntimeError(f\"There are no images in the provided directory: {cfg['test_data_dir']}\")\n",
    "\n",
    "    f_imread = get_im_reader_for_transforms(transforms)\n",
    "\n",
    "    print(\"Let's check if there are broken images:\")\n",
    "    broken_images = find_broken_images(filenames, f_imread=f_imread)\n",
    "    if broken_images:\n",
    "        raise ValueError(f\"There are images that cannot be open:\\n {broken_images}.\")\n",
    "\n",
    "    dataset = ImageBaseDataset(paths=filenames, transform=transforms, f_imread=f_imread)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset, batch_size=cfg[\"bs_val\"], num_workers=cfg[\"num_workers\"], shuffle=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    extractor = get_extractor_by_cfg(cfg[\"extractor\"])\n",
    "    pl_model = ExtractorModule(extractor=extractor)\n",
    "\n",
    "    trainer_engine_params = parse_engine_params_from_config(cfg)\n",
    "    trainer_engine_params[\"use_distributed_sampler\"] = True\n",
    "    trainer = pl.Trainer(precision=16, **trainer_engine_params)\n",
    "    predictions = trainer.predict(model=pl_model, dataloaders=loader, return_predictions=True)\n",
    "\n",
    "    paths, embeddings = [], []\n",
    "    for prediction in predictions:\n",
    "        paths.extend([filenames[i] for i in prediction[dataset.index_key].tolist()])\n",
    "        embeddings.extend(prediction[pl_model.embeddings_key].tolist())\n",
    "\n",
    "    paths = sync_dicts_ddp({\"key\": list(map(str, paths))}, get_world_size_safe())[\"key\"]\n",
    "    embeddings = sync_dicts_ddp({\"key\": embeddings}, get_world_size_safe())[\"key\"]\n",
    "\n",
    "    \n",
    "    return dict(zip(paths, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0d2c96f-1038-42c0-b738-c76268a3c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best-v1.ckpt  best-v2.ckpt  best-v3.ckpt  best-v4.ckpt\tbest.ckpt\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "826d7cd1-1d06-4c49-a7a3-e03f833d7fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'postfix': 'metric_learning', 'seed': 42, 'image_size': 224, 'accelerator': 'gpu', 'devices': 1, 'num_workers': 4, 'cache_size': 0, 'test_data_dir': '../test/gallery/', 'bs_val': 8, 'extractor': {'name': 'vit', 'args': {'arch': 'vitl14', 'normalise_features': False, 'weights': 'checkpoints/best-v4.ckpt'}}}\n",
      "Let's check if there are broken images:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136710/3169649354.py:12: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  albu.PadIfNeeded(\n",
      "100%|██████████| 4784/4784 [00:04<00:00, 1193.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix <model.model.> was removed from the state dict.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e2375268214dd09478db835dcd8043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_results = extractor_prediction_pipeline(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c1133f5-b6f8-4d38-a684-96794021f4f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4784/4784 [01:13<00:00, 65.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Преобразуем данные в массив numpy\n",
    "paths = list(dict_results.keys())\n",
    "embeddings = np.array(list(dict_results.values()), dtype=np.float32)\n",
    "\n",
    "# Нормализуем эмбеддинги для косинусной близости\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Создаем индекс FAISS для косинусной близости\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])  # IndexFlatIP для внутреннего произведения (косинусная близость)\n",
    "index.add(embeddings)  # Добавляем эмбеддинги в индекс\n",
    "\n",
    "final_result = {}\n",
    "# Выбираем запрашиваемое изображение \n",
    "for query_index in tqdm(range(len(paths))):\n",
    "    query_embedding = embeddings[query_index].reshape(1, -1)\n",
    "    query = str(Path(paths[query_index]).name)\n",
    "    # Ищем ближайшие изображения\n",
    "    k = embeddings.shape[0]  # Количество ближайших соседей (все изображения)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Сортируем результаты по расстоянию (косинусная близость)\n",
    "    sorted_results = [Path(paths[i]).name for i in indices[0]]\n",
    "    final_result[query] = sorted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6e233db-a2da-43f1-b77e-694ce28b0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(list(final_result.items()), columns=[\"image_name\", \"recommendation\"])\n",
    "submission_df[\"recommendation\"] = submission_df[\"recommendation\"].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f5f9b7a-aacd-49a3-a4e1-e4c49b0bff35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>recommendation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000248c80d39c0b6f24acd036015f10b.jpg</td>\n",
       "      <td>[000248c80d39c0b6f24acd036015f10b.jpg, 3b3bb4e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b87d3a508b8713b983b9c438664ed.jpg</td>\n",
       "      <td>[000b87d3a508b8713b983b9c438664ed.jpg, 3247e74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0017f47aaacaf983263fcd13d43d0ad1.jpg</td>\n",
       "      <td>[0017f47aaacaf983263fcd13d43d0ad1.jpg, d1d6333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0019608b3f00de8fe18aff80fd940a42.jpg</td>\n",
       "      <td>[0019608b3f00de8fe18aff80fd940a42.jpg, e11cfab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001cac7590f6173c35e11f6ba28fe6f4.jpg</td>\n",
       "      <td>[001cac7590f6173c35e11f6ba28fe6f4.jpg, e3c83eb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_name  \\\n",
       "0  000248c80d39c0b6f24acd036015f10b.jpg   \n",
       "1  000b87d3a508b8713b983b9c438664ed.jpg   \n",
       "2  0017f47aaacaf983263fcd13d43d0ad1.jpg   \n",
       "3  0019608b3f00de8fe18aff80fd940a42.jpg   \n",
       "4  001cac7590f6173c35e11f6ba28fe6f4.jpg   \n",
       "\n",
       "                                      recommendation  \n",
       "0  [000248c80d39c0b6f24acd036015f10b.jpg, 3b3bb4e...  \n",
       "1  [000b87d3a508b8713b983b9c438664ed.jpg, 3247e74...  \n",
       "2  [0017f47aaacaf983263fcd13d43d0ad1.jpg, d1d6333...  \n",
       "3  [0019608b3f00de8fe18aff80fd940a42.jpg, e11cfab...  \n",
       "4  [001cac7590f6173c35e11f6ba28fe6f4.jpg, e3c83eb...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d53ed1d-5f84-4d2a-983c-efb77eef1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
